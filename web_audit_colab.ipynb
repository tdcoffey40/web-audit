{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7983a16",
   "metadata": {},
   "source": [
    "# Web Audit Tool - Google Colab\n",
    "\n",
    "This notebook sets up and runs a comprehensive web audit tool with AI-powered insights using Ollama and GPT-OSS model. Results are automatically saved to Google Drive with timestamps.\n",
    "\n",
    "## Features:\n",
    "- üîç **Deep Content Analysis**: Full page scraping and content extraction\n",
    "- üìä **Comprehensive SEO Audit**: Meta tags, schema markup, Open Graph\n",
    "- ‚ö° **Performance Analysis**: Load times, resource analysis, Core Web Vitals\n",
    "- üåê **Multi-page Crawling**: Discovers and analyzes internal pages\n",
    "- üìà **Visual Reports**: Charts and graphs for audit results\n",
    "- ü§ñ **AI Insights**: Powered by local LLM models\n",
    "- üíæ **Auto-save**: Results exported to Excel in Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae450f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system dependencies and packages\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-browser chromium-chromedriver wget curl\n",
    "\n",
    "# Install Python packages\n",
    "!pip install selenium beautifulsoup4 requests pandas openpyxl xlsxwriter\n",
    "!pip install lighthouse playwright accessibility-checker\n",
    "!pip install Pillow matplotlib seaborn plotly lxml\n",
    "!pip install validators tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a9efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama\n",
    "!curl -fsSL https://ollama.ai/install.sh | sh\n",
    "\n",
    "# Start Ollama service in background\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "\n",
    "def start_ollama():\n",
    "    env = os.environ.copy()\n",
    "    env['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "    subprocess.Popen(['ollama', 'serve'], env=env, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# Start Ollama in a separate thread\n",
    "ollama_thread = threading.Thread(target=start_ollama, daemon=True)\n",
    "ollama_thread.start()\n",
    "\n",
    "# Wait for Ollama to start\n",
    "time.sleep(15)\n",
    "print(\"‚úÖ Ollama service started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58517104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull GPT-OSS model\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "print(\"üì• Pulling GPT-OSS model (this may take 10-15 minutes)...\")\n",
    "\n",
    "try:\n",
    "    # Try to pull the model\n",
    "    process = subprocess.Popen(['ollama', 'pull', 'gpt-oss'], \n",
    "                              stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    \n",
    "    # Monitor the process\n",
    "    for line in iter(process.stdout.readline, ''):\n",
    "        if line.strip():\n",
    "            print(f\"üì¶ {line.strip()}\")\n",
    "    \n",
    "    process.wait()\n",
    "    \n",
    "    if process.returncode == 0:\n",
    "        print(\"‚úÖ GPT-OSS model pulled successfully!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è GPT-OSS not available, falling back to llama3\")\n",
    "        subprocess.run(['ollama', 'pull', 'llama3'], check=True)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Model pull failed, using llama3: {e}\")\n",
    "    subprocess.run(['ollama', 'pull', 'llama3'], check=True)\n",
    "\n",
    "# List available models\n",
    "result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
    "print(\"\\nüìã Available models:\")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create results directory\n",
    "results_base_path = '/content/drive/MyDrive/Web_Audit_Results'\n",
    "os.makedirs(results_base_path, exist_ok=True)\n",
    "print(f\"üìÅ Results will be saved to: {results_base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Selenium imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "# Set up Chrome driver for Colab\n",
    "def setup_chrome_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    chrome_options.add_argument('--disable-extensions')\n",
    "    chrome_options.add_argument('--disable-plugins')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603bf85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama AI Integration\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "def test_ollama_connection():\n",
    "    \"\"\"Test if Ollama is running and models are available\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            available_models = [model.get('name', '') for model in models]\n",
    "            \n",
    "            # Check for preferred models\n",
    "            if any('gpt-oss' in model for model in available_models):\n",
    "                return True, 'gpt-oss'\n",
    "            elif any('llama3' in model for model in available_models):\n",
    "                return True, 'llama3'\n",
    "            else:\n",
    "                return False, None\n",
    "        return False, None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ollama connection error: {e}\")\n",
    "        return False, None\n",
    "\n",
    "def generate_ai_insights(audit_data, model_name):\n",
    "    \"\"\"Generate AI insights from audit data\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze this website audit and provide actionable recommendations:\n",
    "    \n",
    "    Website: {audit_data.get('url', 'Unknown')}\n",
    "    Title: {audit_data.get('title', 'N/A')}\n",
    "    Load Time: {audit_data.get('load_time_ms', 'N/A')} ms\n",
    "    Page Size: {audit_data.get('page_size_kb', 'N/A')} KB\n",
    "    \n",
    "    SEO Analysis:\n",
    "    - H1 tags: {audit_data.get('h1_count', 0)}\n",
    "    - H2 tags: {audit_data.get('h2_count', 0)}\n",
    "    - Meta description: {audit_data.get('meta_description_length', 0)} characters\n",
    "    - Images without alt: {audit_data.get('images_no_alt', 0)}\n",
    "    \n",
    "    Technical Issues:\n",
    "    - Broken links: {audit_data.get('broken_links', 0)}\n",
    "    - Missing viewport: {'Yes' if not audit_data.get('has_viewport') else 'No'}\n",
    "    - HTTPS: {'Yes' if audit_data.get('is_https') else 'No'}\n",
    "    \n",
    "    Provide 5 specific, actionable recommendations to improve this website.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.1,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f\"{OLLAMA_BASE_URL}/api/generate\", \n",
    "                               json=payload, timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('response', 'No insights generated')\n",
    "        else:\n",
    "            return f\"Error generating insights: {response.status_code}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error generating insights: {e}\"\n",
    "\n",
    "# Test connection\n",
    "connected, model = test_ollama_connection()\n",
    "if connected:\n",
    "    print(f\"‚úÖ Ollama connected with model: {model}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ollama not available - AI insights will be disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ee3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Web Audit Functions\n",
    "import validators\n",
    "import tldextract\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class ComprehensiveWebAuditor:\n",
    "    def __init__(self, max_pages=5, timeout=30):\n",
    "        self.driver = None\n",
    "        self.session = requests.Session()\n",
    "        self.results = {}\n",
    "        self.max_pages = max_pages\n",
    "        self.timeout = timeout\n",
    "        self.discovered_pages = set()\n",
    "        self.crawled_pages = []\n",
    "        \n",
    "    def start_session(self):\n",
    "        \"\"\"Initialize browser session\"\"\"\n",
    "        self.driver = setup_chrome_driver()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        \n",
    "    def end_session(self):\n",
    "        \"\"\"Close browser session\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "        self.session.close()\n",
    "            \n",
    "    def audit_website(self, url):\n",
    "        \"\"\"Perform comprehensive website audit\"\"\"\n",
    "        \n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'https://' + url\n",
    "            \n",
    "        if not validators.url(url):\n",
    "            return {'error': f'Invalid URL: {url}'}\n",
    "            \n",
    "        print(f\"üîç Starting comprehensive audit for: {url}\")\n",
    "        \n",
    "        try:\n",
    "            # Initialize results structure\n",
    "            self.results = {\n",
    "                'audit_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                'primary_url': url,\n",
    "                'pages_analyzed': [],\n",
    "                'summary_metrics': {},\n",
    "                'seo_analysis': {},\n",
    "                'performance_metrics': {},\n",
    "                'content_analysis': {},\n",
    "                'technical_analysis': {},\n",
    "                'accessibility_analysis': {},\n",
    "                'security_analysis': {},\n",
    "                'issues_found': [],\n",
    "                'recommendations': []\n",
    "            }\n",
    "            \n",
    "            # Discover internal pages\n",
    "            print(\"üï∑Ô∏è Discovering internal pages...\")\n",
    "            self._discover_pages(url)\n",
    "            \n",
    "            # Audit each discovered page\n",
    "            pages_to_audit = list(self.discovered_pages)[:self.max_pages]\n",
    "            print(f\"üìÑ Analyzing {len(pages_to_audit)} pages...\")\n",
    "            \n",
    "            for i, page_url in enumerate(pages_to_audit, 1):\n",
    "                print(f\"  üìã Page {i}/{len(pages_to_audit)}: {page_url}\")\n",
    "                page_results = self._audit_single_page(page_url)\n",
    "                self.crawled_pages.append(page_results)\n",
    "                \n",
    "            # Compile comprehensive results\n",
    "            self._compile_results()\n",
    "            \n",
    "            print(\"‚úÖ Comprehensive audit completed successfully\")\n",
    "            return self.results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Audit failed: {e}\")\n",
    "            return {'error': str(e), 'url': url}\n",
    "    \n",
    "    def _discover_pages(self, start_url):\n",
    "        \"\"\"Discover internal pages through sitemap and crawling\"\"\"\n",
    "        domain = tldextract.extract(start_url).registered_domain\n",
    "        self.discovered_pages.add(start_url)\n",
    "        \n",
    "        try:\n",
    "            # Check for sitemap\n",
    "            sitemap_urls = [\n",
    "                f\"{start_url}/sitemap.xml\",\n",
    "                f\"{start_url}/sitemap_index.xml\",\n",
    "                f\"{start_url}/robots.txt\"\n",
    "            ]\n",
    "            \n",
    "            for sitemap_url in sitemap_urls:\n",
    "                try:\n",
    "                    response = self.session.get(sitemap_url, timeout=10)\n",
    "                    if response.status_code == 200:\n",
    "                        if 'sitemap.xml' in sitemap_url:\n",
    "                            self._parse_sitemap(response.text, domain)\n",
    "                        elif 'robots.txt' in sitemap_url:\n",
    "                            self._parse_robots(response.text, start_url, domain)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Crawl homepage for internal links\n",
    "            self._crawl_for_links(start_url, domain)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Page discovery error: {e}\")\n",
    "    \n",
    "    def _parse_sitemap(self, sitemap_content, domain):\n",
    "        \"\"\"Parse sitemap XML for URLs\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(sitemap_content, 'xml')\n",
    "            for loc in soup.find_all('loc'):\n",
    "                url = loc.text.strip()\n",
    "                if domain in url and len(self.discovered_pages) < self.max_pages * 2:\n",
    "                    self.discovered_pages.add(url)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def _parse_robots(self, robots_content, base_url, domain):\n",
    "        \"\"\"Parse robots.txt for sitemap references\"\"\"\n",
    "        try:\n",
    "            for line in robots_content.split('\\n'):\n",
    "                if line.lower().startswith('sitemap:'):\n",
    "                    sitemap_url = line.split(':', 1)[1].strip()\n",
    "                    response = self.session.get(sitemap_url, timeout=10)\n",
    "                    if response.status_code == 200:\n",
    "                        self._parse_sitemap(response.text, domain)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def _crawl_for_links(self, url, domain):\n",
    "        \"\"\"Crawl page for internal links\"\"\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            \n",
    "            links = self.driver.find_elements(By.TAG_NAME, \"a\")\n",
    "            for link in links:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if href and domain in href and len(self.discovered_pages) < self.max_pages * 2:\n",
    "                    self.discovered_pages.add(href)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Link crawling error: {e}\")\n",
    "    \n",
    "    def _audit_single_page(self, url):\n",
    "        \"\"\"Perform detailed audit of a single page\"\"\"\n",
    "        page_results = {\n",
    "            'url': url,\n",
    "            'audit_timestamp': datetime.now().isoformat(),\n",
    "            'load_metrics': {},\n",
    "            'seo_data': {},\n",
    "            'content_data': {},\n",
    "            'technical_data': {},\n",
    "            'accessibility_data': {},\n",
    "            'issues': [],\n",
    "            'page_source': ''\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Load page and measure performance\n",
    "            start_time = time.time()\n",
    "            self.driver.get(url)\n",
    "            \n",
    "            # Wait for page load\n",
    "            WebDriverWait(self.driver, self.timeout).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            \n",
    "            load_time = (time.time() - start_time) * 1000\n",
    "            page_results['load_metrics']['load_time_ms'] = round(load_time, 2)\n",
    "            \n",
    "            # Get page source for BeautifulSoup analysis\n",
    "            page_source = self.driver.page_source\n",
    "            page_results['page_source'] = page_source[:1000] + '...' if len(page_source) > 1000 else page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            \n",
    "            # Comprehensive analysis\n",
    "            page_results['seo_data'] = self._analyze_seo(soup)\n",
    "            page_results['content_data'] = self._analyze_content(soup)\n",
    "            page_results['technical_data'] = self._analyze_technical(soup)\n",
    "            page_results['accessibility_data'] = self._analyze_accessibility(soup)\n",
    "            page_results['load_metrics'].update(self._analyze_performance())\n",
    "            \n",
    "            # Identify issues\n",
    "            page_results['issues'] = self._identify_page_issues(page_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            page_results['error'] = str(e)\n",
    "            print(f\"‚ö†Ô∏è Error auditing {url}: {e}\")\n",
    "        \n",
    "        return page_results\n",
    "    \n",
    "    def _analyze_seo(self, soup):\n",
    "        \"\"\"Comprehensive SEO analysis\"\"\"\n",
    "        seo_data = {}\n",
    "        \n",
    "        # Title analysis\n",
    "        title_tag = soup.find('title')\n",
    "        if title_tag:\n",
    "            title_text = title_tag.get_text().strip()\n",
    "            seo_data['title'] = title_text\n",
    "            seo_data['title_length'] = len(title_text)\n",
    "            seo_data['title_optimized'] = 30 <= len(title_text) <= 60\n",
    "        else:\n",
    "            seo_data['title'] = 'Missing'\n",
    "            seo_data['title_length'] = 0\n",
    "            seo_data['title_optimized'] = False\n",
    "        \n",
    "        # Meta tags analysis\n",
    "        meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "        if meta_desc and meta_desc.get('content'):\n",
    "            desc_content = meta_desc['content'].strip()\n",
    "            seo_data['meta_description'] = desc_content\n",
    "            seo_data['meta_description_length'] = len(desc_content)\n",
    "            seo_data['meta_desc_optimized'] = 120 <= len(desc_content) <= 160\n",
    "        else:\n",
    "            seo_data['meta_description'] = 'Missing'\n",
    "            seo_data['meta_description_length'] = 0\n",
    "            seo_data['meta_desc_optimized'] = False\n",
    "        \n",
    "        # Keywords\n",
    "        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
    "        seo_data['meta_keywords'] = meta_keywords.get('content', 'Not specified') if meta_keywords else 'Not specified'\n",
    "        \n",
    "        # Heading structure\n",
    "        headings = {}\n",
    "        for i in range(1, 7):\n",
    "            h_tags = soup.find_all(f'h{i}')\n",
    "            headings[f'h{i}_count'] = len(h_tags)\n",
    "            headings[f'h{i}_text'] = [h.get_text().strip() for h in h_tags[:3]]  # First 3\n",
    "        \n",
    "        seo_data.update(headings)\n",
    "        seo_data['proper_h1_usage'] = headings['h1_count'] == 1\n",
    "        \n",
    "        # Open Graph tags\n",
    "        og_tags = {}\n",
    "        for meta in soup.find_all('meta', property=lambda x: x and x.startswith('og:')):\n",
    "            og_tags[meta.get('property')] = meta.get('content')\n",
    "        seo_data['open_graph'] = og_tags\n",
    "        \n",
    "        # Twitter Card tags\n",
    "        twitter_tags = {}\n",
    "        for meta in soup.find_all('meta', attrs={'name': lambda x: x and x.startswith('twitter:')}):\n",
    "            twitter_tags[meta.get('name')] = meta.get('content')\n",
    "        seo_data['twitter_cards'] = twitter_tags\n",
    "        \n",
    "        # Schema markup\n",
    "        schema_scripts = soup.find_all('script', type='application/ld+json')\n",
    "        seo_data['schema_markup_count'] = len(schema_scripts)\n",
    "        seo_data['has_schema'] = len(schema_scripts) > 0\n",
    "        \n",
    "        # Image SEO\n",
    "        images = soup.find_all('img')\n",
    "        images_with_alt = [img for img in images if img.get('alt')]\n",
    "        seo_data['total_images'] = len(images)\n",
    "        seo_data['images_with_alt'] = len(images_with_alt)\n",
    "        seo_data['images_without_alt'] = len(images) - len(images_with_alt)\n",
    "        seo_data['alt_text_score'] = round(len(images_with_alt) / max(len(images), 1) * 100, 2)\n",
    "        \n",
    "        return seo_data\n",
    "    \n",
    "    def _analyze_content(self, soup):\n",
    "        \"\"\"Comprehensive content analysis\"\"\"\n",
    "        content_data = {}\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Text content analysis\n",
    "        text_content = soup.get_text()\n",
    "        words = text_content.split()\n",
    "        word_count = len(words)\n",
    "        \n",
    "        content_data['word_count'] = word_count\n",
    "        content_data['character_count'] = len(text_content)\n",
    "        content_data['reading_time_minutes'] = round(word_count / 200, 1)  # Average reading speed\n",
    "        \n",
    "        # Content quality indicators\n",
    "        content_data['content_quality_score'] = min(word_count / 300 * 100, 100)\n",
    "        \n",
    "        # Link analysis\n",
    "        links = soup.find_all('a', href=True)\n",
    "        internal_links = []\n",
    "        external_links = []\n",
    "        \n",
    "        current_domain = tldextract.extract(self.driver.current_url).registered_domain\n",
    "        \n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(('http://', 'https://')):\n",
    "                link_domain = tldextract.extract(href).registered_domain\n",
    "                if link_domain == current_domain:\n",
    "                    internal_links.append(href)\n",
    "                else:\n",
    "                    external_links.append(href)\n",
    "            elif href.startswith('/') or not href.startswith(('mailto:', 'tel:', '#')):\n",
    "                internal_links.append(href)\n",
    "        \n",
    "        content_data['total_links'] = len(links)\n",
    "        content_data['internal_links'] = len(internal_links)\n",
    "        content_data['external_links'] = len(external_links)\n",
    "        content_data['internal_link_list'] = internal_links[:10]  # First 10\n",
    "        content_data['external_link_list'] = external_links[:10]  # First 10\n",
    "        \n",
    "        # Media content\n",
    "        content_data['video_count'] = len(soup.find_all('video'))\n",
    "        content_data['audio_count'] = len(soup.find_all('audio'))\n",
    "        content_data['iframe_count'] = len(soup.find_all('iframe'))\n",
    "        \n",
    "        # Lists and tables\n",
    "        content_data['list_count'] = len(soup.find_all(['ul', 'ol']))\n",
    "        content_data['table_count'] = len(soup.find_all('table'))\n",
    "        \n",
    "        return content_data\n",
    "    \n",
    "    def _analyze_technical(self, soup):\n",
    "        \"\"\"Technical SEO and structure analysis\"\"\"\n",
    "        technical_data = {}\n",
    "        \n",
    "        # URL structure\n",
    "        current_url = self.driver.current_url\n",
    "        technical_data['url'] = current_url\n",
    "        technical_data['is_https'] = current_url.startswith('https://')\n",
    "        technical_data['url_length'] = len(current_url)\n",
    "        technical_data['url_clean'] = '?' not in current_url and '#' not in current_url\n",
    "        \n",
    "        # Meta tags\n",
    "        viewport = soup.find('meta', attrs={'name': 'viewport'})\n",
    "        technical_data['has_viewport'] = viewport is not None\n",
    "        technical_data['viewport_content'] = viewport.get('content') if viewport else 'Missing'\n",
    "        \n",
    "        # Canonical URL\n",
    "        canonical = soup.find('link', rel='canonical')\n",
    "        technical_data['has_canonical'] = canonical is not None\n",
    "        technical_data['canonical_url'] = canonical.get('href') if canonical else 'Missing'\n",
    "        \n",
    "        # Robots meta\n",
    "        robots = soup.find('meta', attrs={'name': 'robots'})\n",
    "        technical_data['robots_meta'] = robots.get('content') if robots else 'Not specified'\n",
    "        \n",
    "        # Language\n",
    "        html_tag = soup.find('html')\n",
    "        technical_data['html_lang'] = html_tag.get('lang') if html_tag else 'Not specified'\n",
    "        \n",
    "        # Favicon\n",
    "        favicon = soup.find('link', rel=['icon', 'shortcut icon'])\n",
    "        technical_data['has_favicon'] = favicon is not None\n",
    "        \n",
    "        # CSS and JS resources\n",
    "        stylesheets = soup.find_all('link', rel='stylesheet')\n",
    "        scripts = soup.find_all('script')\n",
    "        \n",
    "        technical_data['stylesheet_count'] = len(stylesheets)\n",
    "        technical_data['script_count'] = len(scripts)\n",
    "        technical_data['inline_styles'] = len(soup.find_all(style=True))\n",
    "        \n",
    "        # Page size\n",
    "        page_size = len(self.driver.page_source.encode('utf-8'))\n",
    "        technical_data['page_size_bytes'] = page_size\n",
    "        technical_data['page_size_kb'] = round(page_size / 1024, 2)\n",
    "        \n",
    "        return technical_data\n",
    "    \n",
    "    def _analyze_accessibility(self, soup):\n",
    "        \"\"\"Accessibility analysis\"\"\"\n",
    "        accessibility_data = {}\n",
    "        \n",
    "        # Form accessibility\n",
    "        forms = soup.find_all('form')\n",
    "        inputs = soup.find_all('input')\n",
    "        labels = soup.find_all('label')\n",
    "        \n",
    "        accessibility_data['form_count'] = len(forms)\n",
    "        accessibility_data['input_count'] = len(inputs)\n",
    "        accessibility_data['label_count'] = len(labels)\n",
    "        \n",
    "        # ARIA attributes\n",
    "        aria_elements = soup.find_all(attrs={'aria-label': True})\n",
    "        aria_elements.extend(soup.find_all(attrs={'aria-labelledby': True}))\n",
    "        aria_elements.extend(soup.find_all(attrs={'role': True}))\n",
    "        \n",
    "        accessibility_data['aria_elements'] = len(set(aria_elements))\n",
    "        \n",
    "        # Alt text for images\n",
    "        images = soup.find_all('img')\n",
    "        images_with_alt = [img for img in images if img.get('alt')]\n",
    "        accessibility_data['image_alt_coverage'] = round(len(images_with_alt) / max(len(images), 1) * 100, 2)\n",
    "        \n",
    "        # Skip links\n",
    "        skip_links = soup.find_all('a', href=lambda x: x and x.startswith('#'))\n",
    "        accessibility_data['skip_links'] = len(skip_links)\n",
    "        \n",
    "        # Color contrast (basic check)\n",
    "        accessibility_data['has_css'] = len(soup.find_all('link', rel='stylesheet')) > 0\n",
    "        \n",
    "        return accessibility_data\n",
    "    \n",
    "    def _analyze_performance(self):\n",
    "        \"\"\"Performance metrics analysis\"\"\"\n",
    "        performance_data = {}\n",
    "        \n",
    "        try:\n",
    "            # Navigation timing\n",
    "            timing = self.driver.execute_script(\"\"\"\n",
    "                var timing = performance.timing;\n",
    "                var navigation = performance.navigation;\n",
    "                return {\n",
    "                    'dns_lookup': timing.domainLookupEnd - timing.domainLookupStart,\n",
    "                    'tcp_connect': timing.connectEnd - timing.connectStart,\n",
    "                    'server_response': timing.responseEnd - timing.requestStart,\n",
    "                    'dom_processing': timing.domComplete - timing.domLoading,\n",
    "                    'total_load_time': timing.loadEventEnd - timing.navigationStart,\n",
    "                    'navigation_type': navigation.type,\n",
    "                    'redirect_count': navigation.redirectCount\n",
    "                };\n",
    "            \"\"\")\n",
    "            \n",
    "            performance_data.update(timing)\n",
    "            \n",
    "            # Resource timing\n",
    "            resources = self.driver.execute_script(\"\"\"\n",
    "                return performance.getEntriesByType('resource').map(function(r) {\n",
    "                    return {\n",
    "                        'name': r.name,\n",
    "                        'type': r.initiatorType,\n",
    "                        'duration': r.duration,\n",
    "                        'size': r.transferSize || 0\n",
    "                    };\n",
    "                });\n",
    "            \"\"\")\n",
    "            \n",
    "            if resources:\n",
    "                resource_summary = defaultdict(list)\n",
    "                for resource in resources:\n",
    "                    resource_summary[resource['type']].append(resource)\n",
    "                \n",
    "                performance_data['resource_summary'] = {\n",
    "                    rtype: {\n",
    "                        'count': len(resources),\n",
    "                        'total_size': sum(r['size'] for r in resources),\n",
    "                        'avg_duration': sum(r['duration'] for r in resources) / len(resources)\n",
    "                    }\n",
    "                    for rtype, resources in resource_summary.items()\n",
    "                }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Performance analysis error: {e}\")\n",
    "        \n",
    "        return performance_data\n",
    "    \n",
    "    def _identify_page_issues(self, page_results):\n",
    "        \"\"\"Identify issues and recommendations for a page\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        seo = page_results.get('seo_data', {})\n",
    "        technical = page_results.get('technical_data', {})\n",
    "        content = page_results.get('content_data', {})\n",
    "        load_metrics = page_results.get('load_metrics', {})\n",
    "        \n",
    "        # SEO issues\n",
    "        if not seo.get('title_optimized', False):\n",
    "            issues.append({\n",
    "                'type': 'SEO',\n",
    "                'severity': 'High',\n",
    "                'issue': f\"Title length ({seo.get('title_length', 0)} chars) not optimal (30-60 chars)\",\n",
    "                'recommendation': 'Optimize title length to 30-60 characters'\n",
    "            })\n",
    "        \n",
    "        if not seo.get('meta_desc_optimized', False):\n",
    "            issues.append({\n",
    "                'type': 'SEO',\n",
    "                'severity': 'High',\n",
    "                'issue': f\"Meta description length ({seo.get('meta_description_length', 0)} chars) not optimal (120-160 chars)\",\n",
    "                'recommendation': 'Write compelling meta description between 120-160 characters'\n",
    "            })\n",
    "        \n",
    "        if not seo.get('proper_h1_usage', False):\n",
    "            issues.append({\n",
    "                'type': 'SEO',\n",
    "                'severity': 'Medium',\n",
    "                'issue': f\"Improper H1 usage ({seo.get('h1_count', 0)} H1 tags found)\",\n",
    "                'recommendation': 'Use exactly one H1 tag per page'\n",
    "            })\n",
    "        \n",
    "        # Technical issues\n",
    "        if not technical.get('is_https', False):\n",
    "            issues.append({\n",
    "                'type': 'Security',\n",
    "                'severity': 'High',\n",
    "                'issue': 'Page not served over HTTPS',\n",
    "                'recommendation': 'Implement SSL certificate and redirect HTTP to HTTPS'\n",
    "            })\n",
    "        \n",
    "        if not technical.get('has_viewport', False):\n",
    "            issues.append({\n",
    "                'type': 'Mobile',\n",
    "                'severity': 'High',\n",
    "                'issue': 'Missing viewport meta tag',\n",
    "                'recommendation': 'Add viewport meta tag for mobile responsiveness'\n",
    "            })\n",
    "        \n",
    "        # Performance issues\n",
    "        load_time = load_metrics.get('load_time_ms', 0)\n",
    "        if load_time > 3000:\n",
    "            issues.append({\n",
    "                'type': 'Performance',\n",
    "                'severity': 'High',\n",
    "                'issue': f'Slow page load time ({load_time}ms)',\n",
    "                'recommendation': 'Optimize images, reduce server response time, enable compression'\n",
    "            })\n",
    "        \n",
    "        # Content issues\n",
    "        if content.get('word_count', 0) < 300:\n",
    "            issues.append({\n",
    "                'type': 'Content',\n",
    "                'severity': 'Medium',\n",
    "                'issue': f\"Low word count ({content.get('word_count', 0)} words)\",\n",
    "                'recommendation': 'Add more valuable content (aim for 300+ words)'\n",
    "            })\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def _compile_results(self):\n",
    "        \"\"\"Compile comprehensive results from all pages\"\"\"\n",
    "        if not self.crawled_pages:\n",
    "            return\n",
    "        \n",
    "        # Summary metrics\n",
    "        self.results['summary_metrics'] = {\n",
    "            'total_pages_analyzed': len(self.crawled_pages),\n",
    "            'avg_load_time': round(sum(page.get('load_metrics', {}).get('load_time_ms', 0) \n",
    "                                     for page in self.crawled_pages) / len(self.crawled_pages), 2),\n",
    "            'total_issues_found': sum(len(page.get('issues', [])) for page in self.crawled_pages),\n",
    "            'pages_with_issues': len([page for page in self.crawled_pages if page.get('issues')]),\n",
    "            'avg_word_count': round(sum(page.get('content_data', {}).get('word_count', 0) \n",
    "                                      for page in self.crawled_pages) / len(self.crawled_pages), 0)\n",
    "        }\n",
    "        \n",
    "        # Aggregate SEO data\n",
    "        all_issues = []\n",
    "        for page in self.crawled_pages:\n",
    "            all_issues.extend(page.get('issues', []))\n",
    "        \n",
    "        issue_counts = Counter(issue['type'] for issue in all_issues)\n",
    "        severity_counts = Counter(issue['severity'] for issue in all_issues)\n",
    "        \n",
    "        self.results['issues_summary'] = {\n",
    "            'by_type': dict(issue_counts),\n",
    "            'by_severity': dict(severity_counts),\n",
    "            'all_issues': all_issues\n",
    "        }\n",
    "        \n",
    "        # Store detailed page results\n",
    "        self.results['pages_analyzed'] = self.crawled_pages\n",
    "\n",
    "print(\"‚úÖ Comprehensive WebAuditor class created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73676dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Results saving and reporting functions\n",
    "def create_comprehensive_report(audit_results):\n",
    "    \"\"\"Create comprehensive visual and text reports\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Extract domain for filename\n",
    "    primary_url = audit_results.get('primary_url', 'unknown')\n",
    "    domain = tldextract.extract(primary_url).registered_domain or 'unknown'\n",
    "    \n",
    "    # Create Excel report with multiple sheets\n",
    "    filename = f\"comprehensive_audit_{domain}_{timestamp}.xlsx\"\n",
    "    filepath = os.path.join(results_base_path, filename)\n",
    "    \n",
    "    with pd.ExcelWriter(filepath, engine='xlsxwriter') as writer:\n",
    "        workbook = writer.book\n",
    "        \n",
    "        # Define formats\n",
    "        header_format = workbook.add_format({\n",
    "            'bold': True, 'bg_color': '#D7E4BC', 'border': 1, 'text_wrap': True\n",
    "        })\n",
    "        \n",
    "        critical_format = workbook.add_format({\n",
    "            'bg_color': '#FFE6E6', 'border': 1\n",
    "        })\n",
    "        \n",
    "        warning_format = workbook.add_format({\n",
    "            'bg_color': '#FFF2E6', 'border': 1\n",
    "        })\n",
    "        \n",
    "        good_format = workbook.add_format({\n",
    "            'bg_color': '#E6F7E6', 'border': 1\n",
    "        })\n",
    "        \n",
    "        # 1. Executive Summary\n",
    "        summary_data = {\n",
    "            'Metric': [\n",
    "                'Audit Date', 'Primary URL', 'Pages Analyzed', 'Total Issues Found',\n",
    "                'Average Load Time (ms)', 'Pages with Issues', 'Average Word Count'\n",
    "            ],\n",
    "            'Value': [\n",
    "                audit_results.get('audit_date', 'N/A'),\n",
    "                audit_results.get('primary_url', 'N/A'),\n",
    "                audit_results.get('summary_metrics', {}).get('total_pages_analyzed', 0),\n",
    "                audit_results.get('summary_metrics', {}).get('total_issues_found', 0),\n",
    "                audit_results.get('summary_metrics', {}).get('avg_load_time', 0),\n",
    "                audit_results.get('summary_metrics', {}).get('pages_with_issues', 0),\n",
    "                audit_results.get('summary_metrics', {}).get('avg_word_count', 0)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_excel(writer, sheet_name='Executive Summary', index=False)\n",
    "        \n",
    "        # Format summary sheet\n",
    "        summary_sheet = writer.sheets['Executive Summary']\n",
    "        summary_sheet.set_column('A:A', 25)\n",
    "        summary_sheet.set_column('B:B', 40)\n",
    "        \n",
    "        for col_num, value in enumerate(summary_df.columns.values):\n",
    "            summary_sheet.write(0, col_num, value, header_format)\n",
    "        \n",
    "        # 2. Issues by Page\n",
    "        issues_data = []\n",
    "        pages_analyzed = audit_results.get('pages_analyzed', [])\n",
    "        \n",
    "        for page in pages_analyzed:\n",
    "            page_url = page.get('url', 'Unknown')\n",
    "            for issue in page.get('issues', []):\n",
    "                issues_data.append({\n",
    "                    'Page URL': page_url,\n",
    "                    'Issue Type': issue.get('type', 'Unknown'),\n",
    "                    'Severity': issue.get('severity', 'Unknown'),\n",
    "                    'Issue Description': issue.get('issue', 'No description'),\n",
    "                    'Recommendation': issue.get('recommendation', 'No recommendation')\n",
    "                })\n",
    "        \n",
    "        if issues_data:\n",
    "            issues_df = pd.DataFrame(issues_data)\n",
    "            issues_df.to_excel(writer, sheet_name='Issues Found', index=False)\n",
    "            \n",
    "            # Format issues sheet\n",
    "            issues_sheet = writer.sheets['Issues Found']\n",
    "            for col_num, value in enumerate(issues_df.columns.values):\n",
    "                issues_sheet.write(0, col_num, value, header_format)\n",
    "            \n",
    "            # Apply conditional formatting based on severity\n",
    "            for row_num, row in issues_df.iterrows():\n",
    "                severity = row['Severity']\n",
    "                row_format = critical_format if severity == 'High' else warning_format if severity == 'Medium' else good_format\n",
    "                \n",
    "                for col_num in range(len(issues_df.columns)):\n",
    "                    issues_sheet.write(row_num + 1, col_num, row.iloc[col_num], row_format)\n",
    "        \n",
    "        # 3. Page Details\n",
    "        page_details = []\n",
    "        for page in pages_analyzed:\n",
    "            seo_data = page.get('seo_data', {})\n",
    "            content_data = page.get('content_data', {})\n",
    "            technical_data = page.get('technical_data', {})\n",
    "            load_metrics = page.get('load_metrics', {})\n",
    "            \n",
    "            page_details.append({\n",
    "                'URL': page.get('url', 'Unknown'),\n",
    "                'Title': seo_data.get('title', 'Missing'),\n",
    "                'Title Length': seo_data.get('title_length', 0),\n",
    "                'Meta Description Length': seo_data.get('meta_description_length', 0),\n",
    "                'Word Count': content_data.get('word_count', 0),\n",
    "                'Load Time (ms)': load_metrics.get('load_time_ms', 0),\n",
    "                'Page Size (KB)': technical_data.get('page_size_kb', 0),\n",
    "                'Internal Links': content_data.get('internal_links', 0),\n",
    "                'External Links': content_data.get('external_links', 0),\n",
    "                'Images Total': seo_data.get('total_images', 0),\n",
    "                'Images with Alt': seo_data.get('images_with_alt', 0),\n",
    "                'H1 Count': seo_data.get('h1_count', 0),\n",
    "                'Issues Count': len(page.get('issues', []))\n",
    "            })\n",
    "        \n",
    "        if page_details:\n",
    "            details_df = pd.DataFrame(page_details)\n",
    "            details_df.to_excel(writer, sheet_name='Page Details', index=False)\n",
    "            \n",
    "            # Format details sheet\n",
    "            details_sheet = writer.sheets['Page Details']\n",
    "            for col_num, value in enumerate(details_df.columns.values):\n",
    "                details_sheet.write(0, col_num, value, header_format)\n",
    "        \n",
    "        # 4. SEO Analysis Summary\n",
    "        seo_summary = []\n",
    "        for page in pages_analyzed:\n",
    "            seo_data = page.get('seo_data', {})\n",
    "            seo_summary.append({\n",
    "                'URL': page.get('url', 'Unknown'),\n",
    "                'Has Title': 'Yes' if seo_data.get('title', 'Missing') != 'Missing' else 'No',\n",
    "                'Title Optimized': 'Yes' if seo_data.get('title_optimized', False) else 'No',\n",
    "                'Has Meta Description': 'Yes' if seo_data.get('meta_description', 'Missing') != 'Missing' else 'No',\n",
    "                'Meta Desc Optimized': 'Yes' if seo_data.get('meta_desc_optimized', False) else 'No',\n",
    "                'Proper H1 Usage': 'Yes' if seo_data.get('proper_h1_usage', False) else 'No',\n",
    "                'Has Open Graph': 'Yes' if seo_data.get('open_graph', {}) else 'No',\n",
    "                'Has Schema Markup': 'Yes' if seo_data.get('has_schema', False) else 'No',\n",
    "                'Alt Text Score': f\"{seo_data.get('alt_text_score', 0)}%\"\n",
    "            })\n",
    "        \n",
    "        if seo_summary:\n",
    "            seo_df = pd.DataFrame(seo_summary)\n",
    "            seo_df.to_excel(writer, sheet_name='SEO Analysis', index=False)\n",
    "            \n",
    "            # Format SEO sheet\n",
    "            seo_sheet = writer.sheets['SEO Analysis']\n",
    "            for col_num, value in enumerate(seo_df.columns.values):\n",
    "                seo_sheet.write(0, col_num, value, header_format)\n",
    "    \n",
    "    print(f\"üìä Comprehensive report saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def create_visual_charts(audit_results):\n",
    "    \"\"\"Create visual charts for audit results\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Set up the plotting style\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Website Audit Dashboard', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Issues by Type\n",
    "        issues_summary = audit_results.get('issues_summary', {})\n",
    "        by_type = issues_summary.get('by_type', {})\n",
    "        \n",
    "        if by_type:\n",
    "            axes[0, 0].pie(by_type.values(), labels=by_type.keys(), autopct='%1.1f%%')\n",
    "            axes[0, 0].set_title('Issues by Type')\n",
    "        else:\n",
    "            axes[0, 0].text(0.5, 0.5, 'No Issues Found', ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "            axes[0, 0].set_title('Issues by Type')\n",
    "        \n",
    "        # 2. Issues by Severity\n",
    "        by_severity = issues_summary.get('by_severity', {})\n",
    "        \n",
    "        if by_severity:\n",
    "            colors = {'High': '#FF6B6B', 'Medium': '#FFE66D', 'Low': '#4ECDC4'}\n",
    "            severity_colors = [colors.get(sev, '#95A5A6') for sev in by_severity.keys()]\n",
    "            axes[0, 1].bar(by_severity.keys(), by_severity.values(), color=severity_colors)\n",
    "            axes[0, 1].set_title('Issues by Severity')\n",
    "            axes[0, 1].set_ylabel('Number of Issues')\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'No Issues Found', ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "            axes[0, 1].set_title('Issues by Severity')\n",
    "        \n",
    "        # 3. Page Load Times\n",
    "        pages = audit_results.get('pages_analyzed', [])\n",
    "        if pages:\n",
    "            load_times = [page.get('load_metrics', {}).get('load_time_ms', 0) for page in pages]\n",
    "            page_names = [urlparse(page.get('url', '')).path or '/' for page in pages]\n",
    "            \n",
    "            axes[1, 0].bar(range(len(load_times)), load_times, color='skyblue')\n",
    "            axes[1, 0].set_title('Page Load Times (ms)')\n",
    "            axes[1, 0].set_ylabel('Load Time (ms)')\n",
    "            axes[1, 0].set_xticks(range(len(page_names)))\n",
    "            axes[1, 0].set_xticklabels(page_names, rotation=45, ha='right')\n",
    "            \n",
    "            # Add horizontal line for 3-second rule\n",
    "            axes[1, 0].axhline(y=3000, color='red', linestyle='--', alpha=0.7, label='3s target')\n",
    "            axes[1, 0].legend()\n",
    "        \n",
    "        # 4. SEO Score Distribution\n",
    "        if pages:\n",
    "            seo_scores = []\n",
    "            for page in pages:\n",
    "                seo_data = page.get('seo_data', {})\n",
    "                score = 0\n",
    "                score += 20 if seo_data.get('title_optimized', False) else 0\n",
    "                score += 20 if seo_data.get('meta_desc_optimized', False) else 0\n",
    "                score += 20 if seo_data.get('proper_h1_usage', False) else 0\n",
    "                score += 20 if seo_data.get('alt_text_score', 0) > 80 else 0\n",
    "                score += 20 if seo_data.get('has_schema', False) else 0\n",
    "                seo_scores.append(score)\n",
    "            \n",
    "            axes[1, 1].hist(seo_scores, bins=5, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "            axes[1, 1].set_title('SEO Score Distribution')\n",
    "            axes[1, 1].set_xlabel('SEO Score (%)')\n",
    "            axes[1, 1].set_ylabel('Number of Pages')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save chart\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        primary_url = audit_results.get('primary_url', 'unknown')\n",
    "        domain = tldextract.extract(primary_url).registered_domain or 'unknown'\n",
    "        \n",
    "        chart_filename = f\"audit_dashboard_{domain}_{timestamp}.png\"\n",
    "        chart_filepath = os.path.join(results_base_path, chart_filename)\n",
    "        \n",
    "        plt.savefig(chart_filepath, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üìà Dashboard saved to: {chart_filepath}\")\n",
    "        return chart_filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error creating charts: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_comprehensive_report(audit_results):\n",
    "    \"\"\"Generate comprehensive text report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä COMPREHENSIVE WEBSITE AUDIT REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary = audit_results.get('summary_metrics', {})\n",
    "    issues_summary = audit_results.get('issues_summary', {})\n",
    "    \n",
    "    print(f\"üåê Primary URL: {audit_results.get('primary_url', 'N/A')}\")\n",
    "    print(f\"üìÖ Audit Date: {audit_results.get('audit_date', 'N/A')}\")\n",
    "    print(f\"üìÑ Pages Analyzed: {summary.get('total_pages_analyzed', 0)}\")\n",
    "    print(f\"‚ö° Average Load Time: {summary.get('avg_load_time', 0)} ms\")\n",
    "    print(f\"üìù Average Word Count: {summary.get('avg_word_count', 0)}\")\n",
    "    \n",
    "    print(f\"\\nüö® ISSUES SUMMARY:\")\n",
    "    print(f\"  ‚Ä¢ Total Issues Found: {summary.get('total_issues_found', 0)}\")\n",
    "    print(f\"  ‚Ä¢ Pages with Issues: {summary.get('pages_with_issues', 0)}\")\n",
    "    \n",
    "    by_severity = issues_summary.get('by_severity', {})\n",
    "    if by_severity:\n",
    "        print(f\"  ‚Ä¢ High Priority: {by_severity.get('High', 0)}\")\n",
    "        print(f\"  ‚Ä¢ Medium Priority: {by_severity.get('Medium', 0)}\")\n",
    "        print(f\"  ‚Ä¢ Low Priority: {by_severity.get('Low', 0)}\")\n",
    "    \n",
    "    by_type = issues_summary.get('by_type', {})\n",
    "    if by_type:\n",
    "        print(f\"\\nüìä ISSUES BY CATEGORY:\")\n",
    "        for issue_type, count in by_type.items():\n",
    "            print(f\"  ‚Ä¢ {issue_type}: {count}\")\n",
    "    \n",
    "    # Top issues\n",
    "    all_issues = issues_summary.get('all_issues', [])\n",
    "    high_priority = [issue for issue in all_issues if issue.get('severity') == 'High']\n",
    "    \n",
    "    if high_priority:\n",
    "        print(f\"\\nüî• TOP PRIORITY ISSUES:\")\n",
    "        for i, issue in enumerate(high_priority[:5], 1):\n",
    "            print(f\"  {i}. {issue.get('issue', 'Unknown issue')}\")\n",
    "            print(f\"     üí° {issue.get('recommendation', 'No recommendation')}\")\n",
    "    \n",
    "    # Performance insights\n",
    "    pages = audit_results.get('pages_analyzed', [])\n",
    "    if pages:\n",
    "        slow_pages = [page for page in pages \n",
    "                     if page.get('load_metrics', {}).get('load_time_ms', 0) > 3000]\n",
    "        \n",
    "        if slow_pages:\n",
    "            print(f\"\\nüêå SLOW LOADING PAGES:\")\n",
    "            for page in slow_pages[:3]:\n",
    "                url = page.get('url', 'Unknown')\n",
    "                load_time = page.get('load_metrics', {}).get('load_time_ms', 0)\n",
    "                print(f\"  ‚Ä¢ {url}: {load_time} ms\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"‚úÖ Enhanced reporting functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6abd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced main audit function\n",
    "def comprehensive_audit(url, max_pages=5):\n",
    "    \"\"\"Main function to perform comprehensive website audit\"\"\"\n",
    "    \n",
    "    auditor = ComprehensiveWebAuditor(max_pages=max_pages)\n",
    "    \n",
    "    try:\n",
    "        # Start browser session\n",
    "        print(\"üöÄ Starting comprehensive website audit...\")\n",
    "        auditor.start_session()\n",
    "        \n",
    "        # Perform comprehensive audit\n",
    "        results = auditor.audit_website(url)\n",
    "        \n",
    "        if 'error' not in results:\n",
    "            # Generate comprehensive report\n",
    "            generate_comprehensive_report(results)\n",
    "            \n",
    "            # Create visual charts\n",
    "            print(\"\\nüìà Creating visual dashboard...\")\n",
    "            chart_path = create_visual_charts(results)\n",
    "            \n",
    "            # Generate AI insights if available\n",
    "            if connected and model:\n",
    "                print(f\"\\nü§ñ Generating AI insights with {model}...\")\n",
    "                \n",
    "                # Prepare comprehensive data for AI\n",
    "                summary_for_ai = {\n",
    "                    'url': results.get('primary_url'),\n",
    "                    'pages_analyzed': results.get('summary_metrics', {}).get('total_pages_analyzed', 0),\n",
    "                    'total_issues': results.get('summary_metrics', {}).get('total_issues_found', 0),\n",
    "                    'avg_load_time': results.get('summary_metrics', {}).get('avg_load_time', 0),\n",
    "                    'issues_by_type': results.get('issues_summary', {}).get('by_type', {}),\n",
    "                    'issues_by_severity': results.get('issues_summary', {}).get('by_severity', {}),\n",
    "                    'top_issues': [issue.get('issue', '') for issue in \n",
    "                                 results.get('issues_summary', {}).get('all_issues', [])[:5]]\n",
    "                }\n",
    "                \n",
    "                insights = generate_ai_insights(summary_for_ai, model)\n",
    "                results['ai_insights'] = insights\n",
    "                print(f\"\\nüí° AI INSIGHTS:\")\n",
    "                print(insights)\n",
    "            \n",
    "            # Save comprehensive results\n",
    "            print(f\"\\nüíæ Saving comprehensive report...\")\n",
    "            excel_path = create_comprehensive_report(results)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Comprehensive audit completed successfully!\")\n",
    "            print(f\"üìÅ Results saved to Google Drive: {results_base_path}\")\n",
    "            \n",
    "            return results, excel_path, chart_path\n",
    "        else:\n",
    "            print(f\"‚ùå Audit failed: {results['error']}\")\n",
    "            return results, None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Audit error: {e}\")\n",
    "        return {'error': str(e)}, None, None\n",
    "        \n",
    "    finally:\n",
    "        # Always close browser\n",
    "        auditor.end_session()\n",
    "\n",
    "# Batch audit function for multiple websites\n",
    "def batch_comprehensive_audit(urls, max_pages_per_site=3):\n",
    "    \"\"\"Perform comprehensive audit on multiple websites\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for i, url in enumerate(urls, 1):\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"üîç COMPREHENSIVE AUDIT {i}/{len(urls)}: {url}\")\n",
    "        print('='*100)\n",
    "        \n",
    "        try:\n",
    "            results, excel_path, chart_path = comprehensive_audit(url, max_pages=max_pages_per_site)\n",
    "            \n",
    "            # Add file paths to results\n",
    "            if excel_path:\n",
    "                results['excel_report_path'] = excel_path\n",
    "            if chart_path:\n",
    "                results['chart_path'] = chart_path\n",
    "                \n",
    "            all_results.append(results)\n",
    "            \n",
    "            # Brief pause between audits\n",
    "            if i < len(urls):\n",
    "                print(\"\\n‚è≥ Waiting 5 seconds before next audit...\")\n",
    "                time.sleep(5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to audit {url}: {e}\")\n",
    "            all_results.append({\n",
    "                'primary_url': url,\n",
    "                'error': str(e),\n",
    "                'audit_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "    \n",
    "    # Create batch summary report\n",
    "    if all_results:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        batch_filename = f\"batch_audit_summary_{timestamp}.xlsx\"\n",
    "        batch_filepath = os.path.join(results_base_path, batch_filename)\n",
    "        \n",
    "        # Create batch summary\n",
    "        batch_summary = []\n",
    "        for result in all_results:\n",
    "            summary = result.get('summary_metrics', {})\n",
    "            batch_summary.append({\n",
    "                'URL': result.get('primary_url', 'Unknown'),\n",
    "                'Status': 'Success' if 'error' not in result else 'Failed',\n",
    "                'Pages Analyzed': summary.get('total_pages_analyzed', 0),\n",
    "                'Total Issues': summary.get('total_issues_found', 0),\n",
    "                'Avg Load Time (ms)': summary.get('avg_load_time', 0),\n",
    "                'High Priority Issues': result.get('issues_summary', {}).get('by_severity', {}).get('High', 0),\n",
    "                'Error': result.get('error', 'None')\n",
    "            })\n",
    "        \n",
    "        batch_df = pd.DataFrame(batch_summary)\n",
    "        batch_df.to_excel(batch_filepath, index=False)\n",
    "        \n",
    "        print(f\"\\nüìã Batch summary saved to: {batch_filepath}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"üöÄ Comprehensive Web Audit Tool Ready!\")\n",
    "print(\"\\nüìñ Enhanced Usage:\")\n",
    "print(\"  ‚Ä¢ Single comprehensive audit: comprehensive_audit('example.com')\")\n",
    "print(\"  ‚Ä¢ Batch comprehensive audit: batch_comprehensive_audit(['site1.com', 'site2.com'])\")\n",
    "print(\"  ‚Ä¢ Adjust pages per site: comprehensive_audit('example.com', max_pages=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef032c92",
   "metadata": {},
   "source": [
    "## üöÄ How to Use the Enhanced Web Audit Tool\n",
    "\n",
    "### Single Comprehensive Website Audit\n",
    "```python\n",
    "# Comprehensive audit with deep analysis\n",
    "results, excel_path, chart_path = comprehensive_audit('example.com', max_pages=5)\n",
    "```\n",
    "\n",
    "### Batch Comprehensive Audit\n",
    "```python\n",
    "# Audit multiple websites with comprehensive analysis\n",
    "websites = [\n",
    "    'google.com',\n",
    "    'github.com', \n",
    "    'stackoverflow.com'\n",
    "]\n",
    "batch_results = batch_comprehensive_audit(websites, max_pages_per_site=3)\n",
    "```\n",
    "\n",
    "### Enhanced Features:\n",
    "- ‚úÖ **Deep Content Scraping**: Full page content extraction with BeautifulSoup\n",
    "- ‚úÖ **Multi-page Discovery**: Automatically discovers internal pages via sitemap & crawling  \n",
    "- ‚úÖ **Comprehensive SEO Analysis**: Title, meta, headings, alt text, Open Graph, Schema markup\n",
    "- ‚úÖ **Performance Metrics**: Load times, resource analysis, Core Web Vitals simulation\n",
    "- ‚úÖ **Content Analysis**: Word count, reading time, link analysis (internal/external)\n",
    "- ‚úÖ **Technical SEO**: HTTPS, viewport, canonical URLs, robots, favicon, HTML lang\n",
    "- ‚úÖ **Accessibility Check**: Form labels, ARIA attributes, skip links, alt text coverage\n",
    "- ‚úÖ **Issue Detection**: Automatic identification of SEO, performance, and technical issues\n",
    "- ‚úÖ **Visual Dashboard**: Charts showing issues by type/severity, load times, SEO scores\n",
    "- ‚úÖ **AI Insights**: Powered by GPT-OSS/Llama3 via Ollama with comprehensive data\n",
    "- ‚úÖ **Multi-sheet Excel Reports**: Executive summary, issues, page details, SEO analysis\n",
    "- ‚úÖ **Auto-save**: All results saved to Google Drive with timestamps\n",
    "\n",
    "### Report Outputs:\n",
    "- **Excel Report**: Multi-sheet comprehensive analysis\n",
    "- **Visual Dashboard**: Charts and graphs (PNG format)\n",
    "- **Console Report**: Detailed text summary\n",
    "- **AI Insights**: Strategic recommendations\n",
    "\n",
    "### Results Location:\n",
    "All audit results are automatically saved to: `Google Drive/Web_Audit_Results/`\n",
    "\n",
    "### Customization:\n",
    "- `max_pages`: Number of pages to analyze per website (default: 5)\n",
    "- `timeout`: Page load timeout in seconds (default: 30)\n",
    "- Reports include color-coded issue severity (Red=High, Orange=Medium, Green=Low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22005400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test - uncomment to try comprehensive audit\n",
    "# comprehensive_audit('example.com', max_pages=3)\n",
    "\n",
    "# For testing with a real website\n",
    "# comprehensive_audit('https://github.com', max_pages=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
